{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMmRzNS1rDnB",
        "outputId": "73173f5d-1073-4c5f-ac6c-f5793cd91d8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter URL of a Medium article: https://medium.com/@ignacio.de.gregorio.noblejas/google-has-finally-dethroned-chatgpt-87a8f8c10d92\n",
            "Hit rate limit, waiting for 60 seconds...\n",
            "Article saved as ./scraped_articles/google-has-finally-dethroned-chatgpt-87a8f8c10d92.txt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import sys\n",
        "import time\n",
        "\n",
        "# Function to get and parse the Medium article HTML\n",
        "def fetch_article():\n",
        "    url = input(\"Enter URL of a Medium article: \")\n",
        "\n",
        "    if not re.match(r'https?://medium.com/.+', url):\n",
        "        print('Please enter a valid Medium article URL.')\n",
        "        sys.exit(1)\n",
        "\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        if response.status_code == 429:  # Too Many Requests\n",
        "            print(\"Hit rate limit, waiting for 60 seconds...\")\n",
        "            time.sleep(60)\n",
        "            response = requests.get(url, headers=headers)  # Retry after wait\n",
        "\n",
        "        response.raise_for_status()  # Ensure the request was successful\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching the article: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    return BeautifulSoup(response.text, 'html.parser'), url\n",
        "\n",
        "# Function to extract text from paragraphs in the article\n",
        "def extract_text(soup, url):\n",
        "    paragraphs = soup.find_all('p')\n",
        "    text = f\"URL: {url}\\n\\n\"\n",
        "\n",
        "    for para in paragraphs:\n",
        "        text += para.get_text() + \"\\n\\n\"\n",
        "\n",
        "    return text\n",
        "\n",
        "# Function to save the extracted text to a file\n",
        "def save_article(text, url):\n",
        "    folder = './scraped_articles'\n",
        "    if not os.path.exists(folder):\n",
        "        os.mkdir(folder)\n",
        "\n",
        "    filename = os.path.join(folder, f\"{url.split('/')[-1]}.txt\")\n",
        "\n",
        "    with open(filename, 'w', encoding='utf-8') as file:\n",
        "        file.write(text)\n",
        "\n",
        "    print(f\"Article saved as {filename}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    soup, url = fetch_article()\n",
        "    article_text = extract_text(soup, url)\n",
        "    save_article(article_text, url)\n"
      ]
    }
  ]
}